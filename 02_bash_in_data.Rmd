---
title: "02_bashing_data"
author: "JR"
date: "10/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In 01_ we started explporing and downloading data. We ended up with a .tsv
file with all the data information we are interested in. Let's explore how we
can use bash to explore the table and learn some more unix commands.

Please download the TSV file here into a directory called "data". We will 
use this .TSV file in a little bit.

https://www.encodeproject.org/report/?type=Experiment&status=released&assay_slims=DNA+binding&biosample_ontology.term_name=HepG2&assay_title=TF+ChIP-seq&biosample_ontology.classification=cell+line&files.read_length=100&files.read_length=76&files.read_length=75&files.read_length=36&assay_title=Control+ChIP-seq&assay_title=Histone+ChIP-seq&files.run_type=single-ended

Let's start with some of the very basics to get everybody on the same page:

cd - to change directories
ls - list the files
pwd - tells you the pathway you are currently at (very useful actually!)
```
cd ~/
ls
pwd
```
Wait could there be more? let's try:

```
ls -lah
```
Do you see new files and folders that start with .X ? These are invisible files
that are often very important. the -lah is called a flag that gives us more information. Let's see what they are 

```
man ls
#q to escape
```
-l for long
-a for all
-h human readable
basically I typically always use these flags

Now let's make a new file using touch

```
touch test.txt
ls
```
woah a new file shows up! What is in it? lets use Nano

```
nano test.txt
# type whatever
# ctrl x, Y for yes -- then enter -- to exit nano
```

Now let's take a look at the file with cat

```
cat test.txt
```
Cat will print all items in a file and sometimes they are very long. So there are other unix commands to get the head and tail of a file while specificying how many lines you want:

```
head -10 test.txt
tail -10 test.txt
```

Let's make another file and combine two files to highlight other functinality of the cat command
First let's remove (using rm command) the test.txt file and make a new one.

```
rm test.txt
touch test.txt
nano test.txt
"add whatever text"
```
Now we will make a second file and merge the two files into one file with cat.

```
tocuh test2.txt
nano test2.txt
# type whatever
cat test.txt test2.txt > test3.txt
cat test3.txt
```

now let's make a folder to put all these files in use mkdir

```
mkdir practice
ls -lah
```

let's move the files in there 

```
mv test2.txt /practice
mv test3.txt /practice
mv test.txt / practice
ls
```

Nice and tidy. However that could be quite laborious if you wanted to move a bunch of files. So we can use WILDCARDS


http://www.linfo.org/wildcard.html

```
mv *.txt /practice
ls
```
This will move any file that ends in .txt the star means anything counts.

Now let's say you want to move the files back to the orignal directory:

```
cd practice
mv *.txt ../
```

The ../ means one directory up from the current path. If oyu want to go up two 
directories you can just add more ../../ etc...

__________________________________
Imagine how much time one uses typing long file paths in the terminal. For example, in this class we will be using:

/Shares/rinn_class/data/CLASS_2021

That is no fun to type everytime and then go to the directory of interest. 

Good news there is a solution (used by nextflow and just how useful later) its called a sym-link and you have probably used them beofre. 

Let's make a sym-link to the class folder in your home directory 

```
cd ~/
ln -s /Shares/rinn_class/data/CLASS_2021 CLASS
ls
```

Now you see a sim-link called CLASS. You just cd CLASS and voila you are in class :)

Another handy short cut is to see how big a folder is. Often you are downloading to
or working on a folder and need to see if it became bigger or smaller. This is best
done wtih disk usage (du)

```
cd ~/
du -sh ~/
```

The -sh flag is for "s"ummary and "h"uman readable


|||||||||||||||||||||||||||||||||
The pipe
|||||||||||||||||||||||||||||||||


The pipe and xargs are two of the most elegant aspects of BASH. Let's try something
simple, so simple it may end up being used quite often :)

Many times a folder you are indexing may have hundreds or thousands of files.
There is no way we want to count them manually. So we can use the pipe to list (ls) 
the files in a directory and pipe it to word count (wc) to count the number of files.

Let's see:

```
cd ~/
ls | wc -l
```
Here the list output becomes the standard input to the word count owing to the pipe.
Or the pipe passed along the standard output of ls to standard input of wc. The 
-l flag is for the line count. 

# Excercise what is the difference in outcome of these pipe choices?

```
ls -lah | wc -l > file.txt | cat file.txt

#versus#

ls -lah > file.txt | wc -l | cat file.txt
```


Same three commands but in different order -- this is just a simple example of 
how to think along with the standard input and output -- becomes much more relevant
if used in more complex bash scripts. 



#Let's move on the find and replace or the GREP command

||||||||||||||||||||||||||||||||||||||||||||||||
General Regularized Expression Print (GREP)
||||||||||||||||||||||||||||||||||||||||||||||||

Some fun background reading of the story behind GREP:
https://www.quora.com/Where-did-GREP-come-from

It's like the search bar, before there was a search bar. Grep will go look for
the search key in a file. If there is a match then you can return just about anything
in the file. If you have ever done Vlookup in Xcel it maybe familiar in that sense. 
But the reality it is so simple, elegnat and powerful we will use GREP a lot in class.

Let's play with the encode .TSV file we downloaded.

```
head data/encode_awk_lessons.tsv
```

Yikes ok, so you see all the "/" that means it is tab deliminated. We would see
commas if it was a .csv.

So this is not very readable. Let's use GREP to get what we want. Let's say we are 
interested in all the samples that start with POL for POL II or POLR2A, there are 
many ways to spell but we can search for anything that starts with POL.


```
grep -ia 'pol' data/encode_awk_lessons.tsv | wc -l
```
I guess we see that there are 11 enteries for anything matching the text of pol.
We used the 

#-i flag which is very useful to match any type of the same letters.
People spell gene names all kinds of different ways (with and with out capitals etc).
So the -i will match Pol POl and POL as well as poL. Try running with out the -i (no matches!).

#-a is simply saying we are looking for a text match.

Note that we piped to wc -l, if you take that out what happens?

Now open up the same file in XCEL and search for pol -- do you find 11 enteries?

So grep likes to go look but needs to be told where to disseminate what was found!
So let's print the grep standard output to a file using ' > ' 
'>' is a very powerful "pipe" if yoy will to say "take standard out to print"

Let's take a look at these 11 matches.

```
grep -ia 'pol' data/encode_awk_lessons.tsv > grep_out.txt
ls
cat grep_out.txt
```

So we see a new file was printed, but let's open in xcel for ease.

Ok, so this is a great example of how to be careful with grep. We loosened the 
search a bit too much and it turns out some of those weird encode acessions had pol 
in the string! But we do see the samples we want are "POLR2A" -- let's revamp our
grep.



```
grep -aw 'POLR2A' data/encode_awk_lessons.tsv > grep_out.txt
cat grep_out.txt
```
Here the -w requires an exact match and -x means the entire line has to match.
Now we see a file with just the POLR2A experiments -- as we wanted.

Let's say we wanted to know how many unique DBPs we are about to study. We can
bring AWK in, which is like selecting and moving columns in excel. So we could awk
the column with DBP names and put it into "unique" to know the number of unique DBPS.


|||||||||||||||||||||||||||||||||||||||||||||||||||||||
Alfred Aho peter Weinberger brian Kernighan = AWK
|||||||||||||||||||||||||||||||||||||||||||||||||||||||

So we can use AWK in a similar way to grep to get started:

```
man awk
awk -F $'\t' '{if ($6 == "POLR2A") print $0;}' data/encode_awk_lessons.tsv | wc -l 
```

The syntax for awk is:
awk -options 'selection _criteria {action }' input-file > output-file

so here we see the selection criteria is an if statement that if column 6 ($6)
is equal to (exactly ==) the "POLR2A" term. print $0 means to print the lines that
match these arguments and ; means end. Then, somewhat counter intuitive we put the
file we want to operate on and then > output file. 


We see that we get only the two TRUE POLR2A and not the phospho samples. This is 
why grep is better suited for this kind of thing. Let's now use awk to acomplish
the goal of seeing how many unique DBPs are in this file.

First let's make a file with just the names of DBPs.


```
awk 'BEGIN {FS="\t"}; {print $6}' data/encode_awk_lessons.tsv
```
Here we see one of many ways to wrtie the same thing in AWK. The awk arguements
are encapsulated in the ' ' statement(s) inside { }. It is good practice to tell
awk what type of deliminator there are betweeen columns. This being done in the first
statement BEGIN means start with this statement {FS = "\t"}; this is saying the 
Field Seperator (FS) is a tab deliminated file "\t". The semicolon ends this argument.


The next statement is using the funciton print to "print" the column of choice.
In this case we are choosing column 6 as it has the names of all the DBPs.

The final argument after the awk instructions is the file in which to perform the
awk instructions on -- data/encode_awk_lessons.tsv

Cool, but it's all in standard output right now -- let's get back to the quesiton:
How many unique DBPs could we analyze?

To do so we will just take advantage of the pipe and sort command.
```
man sort
```
Let's put it together:

```
awk 'BEGIN {FS="\t"}; {print $6}' data/encode_awk_lessons.tsv | sort -u | wc -l
```
We piped out the awk arguments from standard in put to standard output of sort.
We used the -u flag to sort and then collapse the sort to unique string names.
then we simply count the lines to see if it worked?

YES! How many??

Let's print this out to a file:

```
awk 'BEGIN {FS="\t"}; {print $6}' encode_awk_lessons.tsv > dbp.tsv
```

Let's use awk, grep and pipes to make a new file from the .tsv. Very similar to
above, but now showing the power of awk in doing many things for you at once!

```
awk 'BEGIN {FS="\t"}; {print $1,$2,$3}' data/encode_awk_lessons.tsv > test.txt
```
What happened here? How could you test if it was the right file output?


->O<-->O<-->O<-->O<-->O<-->O<-->O<-->O<-
for loops in BASH
->O<-->O<-->O<-->O<-->O<-->O<-->O<-->O<-

```
for x in $(seq 1 42); do echo BCHM5631 is great $x; done
```

For loops are probably one of the most fundemental aspects of computing. It allows 
us to do many repetitive tasks with only a few instruction.

The syntax in bash is:

For X in $Y do 

#For / in / do / done will always be used in a for loop


Let's start with a really simple example that helps us understand how the computer
thinks about this:


```
For x in 1 1 1 1
do 
echo BCHM5631
done
```


What we get out is 4 prints of BCHM5631 -- what happens if we change the numbers?

```
For x in 1 3 11 14
do 
echo BCHM5631 $x
done
```

Same result, you could even change it to apple, banana or anything -- the number
of objects after "in" are the inputs for each loop until there is nothing more to 
be "in". Notice the added "$x" this means it will print the value it's "in" currently.


Let's use the 'seq' command to set the variables to be "in"

```
for x in $(seq 1 50)
do 
for y in A B C
do
echo "$x:$y"
done
done

#for x in $(seq 1 5); do for y in A B C; do echo "$x:$y"; done ; done
```

Let's try this one:

```
for x in $(seq 1 42)
do 
echo BCHM5631
done

#for x in $(seq 1 42); do echo BCHM5631 is great $x; done
```

Ok so there is a lot going on in this simple little loop. Notice that we didn't 
have to type in 1 2 3 4 5 -- instead we used the $ to set a "variable" or string.
inside the $ is the seq command which will go from 1 until it hits 5. This way 
you can specify a range of your choosing. The other thing we see if the for loop 
starts with 1 and then does another for loop with a new varible "y". In this order the 
loop is going to stage itself in position 1 of the first for loop then go to position
A in the second loop, then to B and C before it returns back to the first loop to 
move to the next position 2.


We would probably rather do a for loop while reading in a file and changing something.
Let's change the text in our encode practice file. 

```
for line in $(cat /data/encode_awk_lessons.tsv)
do 
echo "$line"
done
```
Woah in the blink of an eye we just used cat to print each line of the file :)

We basically just did cat but used a for loop to print one line at a time.

This is identical to do this with "read" command too.

```
while read line
do
echo "$line"
done < data/encode_awk_lessons.tsv
```

Notice we have the same structure but we are putting the input file at end with
< arrow sort of counter intutively putting the input file last .... also line is
still the new variable being made "while" is similar to the "for" statemetnt but 
saying while there is something to read print each line :)

Typically we would want to maniputate or change the name of a column in a file
systematically with "sed". Sed is a simple elegant and powerful unix commnad that
can parse and transform text. Let's give it a spin.

```
man sed
```

This basically distills down to 

sed -i 's/old-word/new-word/g' *.txt 
the s/ is a substitute command 

Let's try changing POLR2A to POL2

```
sed -i 's/$POLR2A/$POL2/g' data/encode_awk_lessons.tsv
```

Did you see a change?
sed is a really nice way to change chromosome annotations that tend to change in time
for example chromosome 1 maybe chr1 C1 or 1 -- in one file and a differnet spelling
in another but if they ever need to connect you will want them in the same format.

one line with sed and golden -- can also use in a for loop to replace many files.


```
for f in $(cat data/encode_awk_lessons.tsv)
do
sed -i 's/USF2/USA/g' data/encode_awk_lessons.tsv
do 
echo "$f"
done

```

This is a very silly for loop becuase sed already kinda has it built in...sed is just 
like find and replace. 


*********************
EXCERCISE
*********************

Use a for loop to create a random set of three groups of 5 students in the class.
What does the input file need to look like 


Beginer version 

```
plist=(1 2 3 4 5)
for i in ${plist[@]}; do
    echo "$i"
done > file.txt
cat file.txt file.txt > file2.txt
cat file2.txt file.txt > file3.txt
paste -d ' ' file3.txt bchm.csv 
#man paste
```
This is pretty simple, but still may have some scary parts. let's break it down:

first we set a variable plist to the numbers 1-5 (for 5 members in the group)
then we start the for loop for the first object i in plist (1,2,3,4 or 5). But as 
we can see it's slightly more complicated we used ${ } to index into our plist.
The computer is sorting 1-5 in a "hash" which is usally indexed with [@]. 

Indexing is very fundemental to what we will do, but don't worry if it seems confusing
right now -- we will do it so much it will become second nature :) 

The first command ends with ; then we invoke the do command in bash. It's doing what 
it says it's doing telling the computer to do what's next in line. Echo simply prints 
what is in standard output currently (i) in the ${plist[@]}. 

Finally we say done to end the loop. 

This simply prints 1-5 into a file.txt. Then I merge the same two files to get 
two sets of 1-5 and a third merge (cat). This is a terrible for loop :) 

Here is a more advanced version from Michael

```
awk -F "," '{print $1}' bchm_5631_enrollment.csv | tail -n +2 > names.txt
```
# we saw this above, now with comma seperator to print column 1. then piped to
# take only the last two enteries and print to names.txt

```
num_students=$(cat names.txt | wc -l)
```
#setting a variable like plist above called num_students

```
RANDOM=45208908
```

# This is setting a random seed so the randomness is even reproducible :)

```
echo -n "" > random_list.txt
```
# NOt sure

```
for i in `seq $num_students`
do
	echo $RANDOM >> random_list.txt
done
```

# The for loop that is using echo to print the current random number so each
# student gets a random number in sets of 5

```
paste -d',' random_list.txt names.txt > names_w_random.txt
```

# printing out the file

```
sort -t" " -n -k2 names_w_random.txt > names_sorted.txt
```
# sorting the output file by random number column

```
awk '{print} NR %3 == 0 { print "" }' names_sorted.txt
```

## NOT SURE



_________________________ Bonus Bash Fun _________________________ 

#### Bonus
pipe with something fun -- download a youtube video
from your terminal:

```
youtube-dl $1 -q -o - | ffmpeg -i - $2
```

downloads a video from the given youtube url passed by $1 and outputs it as the file given by $2. Note how the file is quietly -q output to STDOUT -o -, piped to ffmpeg and used as input there by -i -. The only downside is you have to install ffmpeg -- but it's not that bad:

http://ericholsinger.com/install-ffmpeg-on-a-mac
####




### Bonus. Have you ever had a folder of folders of folders? This is often the case
with photo libraries, music and other large archives. But let's say you simply just
want to retreive all the photos on an app before the app goes extinct etc. 

You can do this very easily with BASH alone: with the powerful find command:


```
man find
find . -type f -name '*.jpg' -exec mv -i {} ../compiled/ \;
```

with this snippet we call find to look in the directory we are in (. = here)
we used the -type flag to look for files with -name that is anything that ends in
.jpg ('*.jpg'). Then the cool stuff starts happening. We call -exec for execute the 
next command. In otherwords standard out put is going to be "piped" into the move
command (mv). So we floated all the file paths ending in .jpg to the mv funciton and
last we just tell the computer where to move the files (or copy (cp)). The back slash
semi-colon ends the bash script. Not so bad -- just standard input and output movements.






